

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Getting started &mdash; HPCortex 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=2709fde1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Accelerator" href="api_code.html" />
    <link rel="prev" title="Dependencies" href="installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            HPCortex
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Dependencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#cuda-openmp-for-nvidia-v100-with-compute-capability-7-0">CUDA + OpenMP (for NVIDIA V100 with compute capability 7.0):</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#pure-openmp">Pure OpenMP:</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#olcf-frontier">OLCF Frontier:</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#alcf-aurora">ALCF Aurora:</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#application-boilerplate-and-compilation">Application boilerplate and compilation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#building-a-simple-model">Building a simple model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training-the-model">Training the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inference">Inference</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_code.html">Accelerator</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#activationfuncs">ActivationFuncs</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#comms">Comms</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#components">Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#ddp">DDP</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#dynamicmodel">DynamicModel</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#embeddings">Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#hpcortex">HPCortex</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#init">Init</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#instancestorage">InstanceStorage</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#layers">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#linalg">Linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#lossfunctions">LossFunctions</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#managedarray">ManagedArray</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#memorymanager">MemoryManager</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#optimizers">Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#padding">Padding</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#performance">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#pipelining">Pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#random">Random</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#ringbuffer">RingBuffer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#serialization">Serialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#tensors">Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#testing">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#timing">Timing</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#batch3tensorpaircontractcomponent">Batch3tensorPairContractComponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#batchedmatrixrowsoftmaxcomponent">BatchedMatrixRowSoftMaxComponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#batchtensorconcatenatecomponent">BatchTensorConcatenateComponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#batchtensordimensionslicecomponent">BatchTensorDimensionSliceComponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#batchtensordnncomponent">BatchTensorDNNcomponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#matrixtensorcontractcomponent">MatrixTensorContractComponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#multiheadattentioncomponent">MultiHeadAttentionComponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#normcomponent">NormComponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#scalecomponent">ScaleComponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#scaleddotproductattentioncomponent">ScaledDotProductAttentionComponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#scaleddotproductattentionheadcomponent">ScaledDotProductAttentionHeadComponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#softmaxcomponent">SoftMaxComponent</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#batchtensordnnlayer">BatchTensorDNNlayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#convolutionlayer1d">ConvolutionLayer1D</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#embedpositionssinusoidallayer">EmbedPositionsSinusoidalLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#flattenlayer">FlattenLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#inputlayer">InputLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#layercommon">LayerCommon</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#matrixtensorcontractlayer">MatrixTensorContractLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#multiheadcrossattentionlayer">MultiHeadCrossAttentionLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#multiheadselfattentionlayer">MultiHeadSelfAttentionLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#normlayer">NormLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#pairjoinlayer">PairJoinLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#pairsplitlayer">PairSplitLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#replicatelayer">ReplicateLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#scaleddotproductselfattentionlayer">ScaledDotProductSelfAttentionLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#skipconnection">SkipConnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#softmaxlayer">SoftMaxLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#sumjoinlayer">SumJoinLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#transformerencoderdecoderblock">TransformerEncoderDecoderBlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_code.html#unflattenlayer">UnflattenLayer</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">HPCortex</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Getting started</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/getting_started.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="getting-started">
<h1>Getting started<a class="headerlink" href="#getting-started" title="Link to this heading"></a></h1>
<p>In this section we will illustrate the fundamentals of building, training and using models in HPCortex via a simple example. The complete working version of this code can be found in <code class="docutils literal notranslate"><span class="pre">examples/example_dnn.cpp</span></code> which is compiled by default.</p>
<section id="application-boilerplate-and-compilation">
<h2>Application boilerplate and compilation<a class="headerlink" href="#application-boilerplate-and-compilation" title="Link to this heading"></a></h2>
<p>To build an application, first include HPCortex’ header, <code class="docutils literal notranslate"><span class="pre">#include&lt;HPCortex.hpp&gt;</span></code>, then initialize the library as follows:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>int main(int argc, char** argv){
	initialize(argc, argv);
</pre></div>
</div>
<p>Compilation is made convenient through HPCortex’ configuration tool, <code class="docutils literal notranslate"><span class="pre">hpcortex-config</span></code>, which stores and provides the build flags determined during configuration. After ensuring <code class="docutils literal notranslate"><span class="pre">/path/to/install/bin</span></code> is in your <code class="docutils literal notranslate"><span class="pre">PATH</span></code> environment variable, simply construct a Makefile from the following template (here for an application <code class="docutils literal notranslate"><span class="pre">train</span></code> with source <code class="docutils literal notranslate"><span class="pre">train.cpp</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>CXX := $(shell hpcortex-config --cxx)
CXXFLAGS := $(shell hpcortex-config --cxxflags)
LDFLAGS := $(shell hpcortex-config --ldflags)
LIBS := $(shell hpcortex-config --libs)

train: train.cpp
	$(CXX) train.cpp $(CXXFLAGS) $(LDFLAGS) $(LIBS) -o train
</pre></div>
</div>
<p>To run simply ensure <code class="docutils literal notranslate"><span class="pre">/path/to/install/lib</span></code> is in your <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> environment variable and execute the binary as normal.</p>
</section>
<section id="building-a-simple-model">
<h2>Building a simple model<a class="headerlink" href="#building-a-simple-model" title="Link to this heading"></a></h2>
<p>Models in HPCortex are structured as nested instances of layers, with the top-most layer being the output layer and the bottom-most the input layer. Layers are conveniently instantiated through wrapper functions that hide away some of the type-passing details and, optionally, initialize the layer’s parameters.</p>
<p>The following simple example creates a fully-connected network with one hidden layer</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>int n_out = 10;
int n_in = 5;
int n_hidden = 25;

auto model = dnn_layer(n_out, n_hidden,
		       dnn_layer(n_hidden, n_in, ReLU&lt;float&gt;(),
				 input_layer&lt;confSingle&gt;()				 
				 )
		       ); 
</pre></div>
</div>
<p>Note the following:</p>
<ul class="simple">
<li><p>In general, the arguments to a layer appear first within the function signature of a layer instantiation function, followed by the layer below.</p></li>
<li><p>The model must always terminate on a <em>single</em> input layer. In this example, the input type is assumed to be a matrix, where the <em>column</em> index is the batch index. As this is the default input type it does not need to be explicitly specified; however, <code class="docutils literal notranslate"><span class="pre">input_layer&lt;confSingle,</span> <span class="pre">Matrix&lt;float&gt;</span> <span class="pre">&gt;()</span></code> is equivalent to the above.</p></li>
<li><p>Batching, or more correctly, <em>mini-batching</em>, is exploited at the fundamental level in HPCortex: All inputs and outputs to the model are batch-processed in parallel for more efficient computation.</p></li>
<li><p>The dimensions of the internal parameter tensors must be specified at this stage for purposes of initialization, or else explicit initial values must be provided. Here we are allowing the parameters to be initialized automatically using the default <a class="reference external" href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Glorot uniform random</a> (also cf. <a class="reference external" href="https://fluxml.ai/Flux.jl/stable/reference/utilities/#Flux.glorot_uniform">here</a>)</p></li>
<li><p>The dimensions of the <em>data</em> (including the batch size) generally do not need to be specified at this stage; rather, the first use of the model will fix these parameters internally.</p></li>
<li><p>The floating point type is specified explicitly as a parameter of the activation function (<code class="docutils literal notranslate"><span class="pre">ReLU</span></code> here) and the input layer through the configuration argument <code class="docutils literal notranslate"><span class="pre">confSingle</span></code> (use <code class="docutils literal notranslate"><span class="pre">confDouble</span></code> for double precision). This type is passed through to the layers above automatically.</p></li>
</ul>
</section>
<section id="training-the-model">
<h2>Training the model<a class="headerlink" href="#training-the-model" title="Link to this heading"></a></h2>
<p>Training first requires the model to be wrapped in a loss function. Here we will use the inbuilt mean-squared error (MSE) loss function that computes the average of the element-wise squared differences between the prediction and the training data, including over the batch dimension. This loss function comes with its own convenience wrapper function:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> auto loss = mse_cost( model );
</pre></div>
</div>
<p>The training process follows the standard approach of minimizing the loss by computing the gradient of the loss with respect to the model parameters, and taking steps along the (negative) gradient direction. HPCortex supports multiple <strong>Optimizers</strong> to perform this minimization. Here we will use the popular Adam optimizer with a specific initial learning rate:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>float learning_rate = 0.005;
AdamOptimizer&lt;float&gt; optimizer(learning_rate);
</pre></div>
</div>
<p>We must next provide a <strong>DataLoader</strong>, whose job it is to provide batches of matched inputs and data. The input data type, once batched, must match the type provided to the input layer of the model specification above, while the data type is whatever is appropriate for the loss function, and does not necessarily have to be the same as the model output type provided the loss function knows how to combine the two.</p>
<p>Below, we generate some training and validation data in the form of a standard vector of <code class="docutils literal notranslate"><span class="pre">XYpair</span></code> containers,</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>template&lt;typename FloatType, int DimX, int DimY&gt;
struct XYpair{
  Tensor&lt;FloatType,DimX&gt; x;
  Tensor&lt;FloatType,DimY&gt; y;
};
</pre></div>
</div>
<p>comprising the paired inputs and data. Data of this form has an inbuilt dataloader, <code class="docutils literal notranslate"><span class="pre">XYpairDataLoader</span></code>, associated with it.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>constexpr int DataDim = 1;
typedef std::vector&lt;XYpair&lt;float,DataDim,DataDim&gt; &gt; XYpairVector;
int ntrain = 200;
int nvalid = 50;  
XYpairVector data = generate_data(ntrain + nvalid);  
XYpairVector train_data(data.begin(),data.begin()+ntrain);
XYpairVector valid_data(data.begin()+ntrain,data.end());

XYpairDataLoader&lt;float,DataDim,DataDim&gt; loader(train_data);
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">generate_data()</span></code> is some (here unspecified) function that generates the data for this example. Note that <code class="docutils literal notranslate"><span class="pre">DataDim</span></code>, the dimension of the data, is one and not two, as the batch dimension is only used internally.</p>
<p>Finally, we can train the model using <code class="docutils literal notranslate"><span class="pre">train</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>int nepoch = 100;
int batch_size = 4;
std::vector&lt;float&gt; loss_history = train(loss, loader, optimizer, nepoch, batch_size);
</pre></div>
</div>
<p>where we provided the number of epochs (complete passes over the training data set) and the batch size. The function returns the batch loss history.</p>
</section>
<section id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Link to this heading"></a></h2>
<p>Once trained, the model can be employed for inference. In this case we will validate the model against the validation data that we separated out above.</p>
<p>At present, the layers in HPCortex often store the batch size internally and test that their inputs conform to this. As a result, the batch size generally cannot be changed once the model is trained. In particular, this means that if we want to perform inference for a single, unbatched input, we must first place it into a batched tensor. For the common case (as it is in our example) where the inputs and outputs are all tensors, the loss-function wrapper (<code class="docutils literal notranslate"><span class="pre">loss</span></code> above) offers a convenience function that performs this operation, <code class="docutils literal notranslate"><span class="pre">predict(value,</span> <span class="pre">batch_size)</span></code>, where value is an unbatched tensor. We will use this below.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>for(int i=0;i&lt;valid_data.size();i++){
  auto const &amp;xy = valid_data[i];
  Vector&lt;float&gt; prediction = loss.predict(xy.x, batch_size); //&lt;--- perform inference
  Vector&lt;float&gt; diff = prediction - xy.y;
  double loss = norm2(diff) / n_out;    
  std::cout &lt;&lt; xy.x &lt;&lt; &quot; -&gt; &quot; &lt;&lt; loss &lt;&lt; std::endl;
}   
</pre></div>
</div>
<p>In this example we manually compute the MSE validation loss and output it to the terminal.</p>
<p>Inference can also be performed by calling <code class="docutils literal notranslate"><span class="pre">loss.predict(batched_input)</span></code> for a batched input tensor, or by calling <code class="docutils literal notranslate"><span class="pre">model.value(batched_input)</span></code> on the model itself.</p>
<!-- The derivative is computed using automatic backwards differentiation.
<!-- .. In the forward pass, each layer receives a tensor from the layer below, which is then processes and passes onto the layer above.
<!--   Likewise, in the backward (derivative) pass, each layer receives a tensor representing the derivative of the loss with respect to the *outputs* of that layer, as well as an array to populate with the derivatives of the loss
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="installation.html" class="btn btn-neutral float-left" title="Dependencies" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="api_code.html" class="btn btn-neutral float-right" title="Accelerator" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>